* Data Mining
  - Course: Data Mining - COMP 3431 2, Data Mining - COMP 4431 1
  - Class time: Tue, Thu  05:00 PM -  06:50 PM  |Engineering & Computer Science | Room 410
  - Instructor: /Pooran Singh Negi, pooran.negi@du.edu/ [[https://sites.google.com/site/poorannegi/][webpage]]
  - Office: 470
  - Office Hours: Tue, Thu,  4.00 p.m. - 4.45 p.m and 6.50 p.m. - 7.35 p.m. Email for 1-on-1 help.
  - GTA: Mitchell Wright, GTA office hours, 3-5 on Monday and 3-5 on Wednesday
  - Prerequisite: *Good understanding of Linear algebra, probability & statistics, calculus and some programming experience in python*
** Books 
*** required
   - [[http://www.dataminingbook.info/pmwiki.php][Data Mining and Analysis: Fundamental Concepts and Algorithms 1st Edition]]
   
Readings from the book will be posted on the course schedule

*** Optional
 - [[http://www-bcf.usc.edu/~gareth/ISL/][An Introduction to Statistical Learning]]
 - [[https://web.stanford.edu/~hastie/ElemStatLearn/][The Elements of Statistical Learning: Data Mining, Inference, and Prediction.]] by Hastie, Tibshirani and Friedman
** Optional material online


* Course Description
It is recommended that you consult this github page often for material related to this course. You should check your e-mail periodically for messages.
Assignments will be upload here and in the [[https://canvas.du.edu/login/ldap][canvas]].

This course will provide introduction to some topics in data mining like classification, regression, clustering and knowledge discovery. We'll also go over
algebraic, statistical and geometric foundations of these topics. 

We'll use [[http://jupyter.org/][jupyter notebook/lab]] for in the class and homeworks assignments. You are expected to learn [[https://www.python.org/][python]] on your own via tutorials etc.
We'll also cover scientific python eco system libraries like numpy, sklearn and matplotlib. *Please see Software installation section to see how to setup computational environment.*


** Syllabus
*This syllabus is subject to change at the discretion of the instructor*.
- pattern mining,
- classification,
- regression
- clustering
- text mining


* Grading
*There will be  written/coding homework assignments, quizzes,  midterm and a final. We'll drop one of your worst assignment grade*.
We'll allow 2 late homework with cutoff of 36 hours. We'll give *ceil(total_marks_obtained*exp(-(minutes late)/(24*60)))* marks  for  late submitted assignments via email.



** Dates

|------------------------------------------------+-----|
| Homework  and may be a final project challenge | 40% |
|------------------------------------------------+-----|
| Quiz                                           | 10% |
|------------------------------------------------+-----|
| Midterm, 7 th May in class                     | 20% |
|------------------------------------------------+-----|
| Final, 11 th June in class                     | 30% |
|------------------------------------------------+-----|



** Final course grading rubric

grade range [('A', >=93), ('A_minus', >=89), ('B_plus', >=85), ('B', >=81), ('B_minus', >=77), ('C_plus', >=73), ('C', >=69), ('C_minus', >=65),
 ('D_plus', >61), ('D', >=57), ('D_minus', >=53),  ('F', < 53)])

 
* Honor code
All members of the University of Denver community are expected to uphold the values of Integrity, Respect, and Responsibility.
These values embody the standards of conduct for students, faculty, staff, and administrators as members of the University community. 
Our institutional values are defined as:

Integrity: acting in an honest and ethical manner;

Respect: honoring differences in people, ideas, experiences, and opinions;

Responsibility: accepting ownership for one's own behavior and conduct.

Please respect DU [[https://www.du.edu/studentlife/studentconduct/honorcode.html][Honor Yourself, Honor the Code]]

* Students with Disabilities
Students with recognized disabilities will be provided reasonable
accommodations, appropriate to the course, upon documentation of the disability with a Student
Accommodation Form from the Disability Services Program. *To receive these accommodations, you must request the specific accommodations, by submitting them to the instructor in writing,
by the end of first week of classes.* Visit [[https://www.du.edu/studentlife/disability/][CAMPUS LIFE & INCLUSIVE EXCELLENCE]] webpage for details.

* Withdrawal Policy
Please see [[https://www.du.edu/registrar/calendar/][registrar calender]] for Academic deadlines. *We'll strictly follow the deadlines.*

 more to come
     
* Software Installation
** Python
We want everybody to have same experience using computational tools in data science tools 1. Please follow steps as
per your operating system.

*** Window based installation
Please install Windows Subsystem for Linux (WSL) on window 10. Follow the instruction in this post [[https://medium.com/hugo-ferreiras-blog/using-windows-subsystem-for-linux-for-data-science-9a8e68d7610c][Using Windows Subsystem for Linux for Data Science]]
by Hugo Ferreira for installing Linux. **ignore install Anaconda part.**

You can also watch this [[https://www.youtube.com/watch?v=Cvrqmq9A3tA][video]] to see installation of Windows 10 Bash & Linux Subsystem Setup.
** Linux /Mac users should already have bash command prompt
You can run *echo $0* to check current shell. Change to bash shell using  *chsh -s /bin/bash*

*One you are in Linux/Mac bash command prompt, Please follow following instructions*
** Python3 installation
Please follow instructions [[https://realpython.com/installing-python/][here]] to install python3 if it is not installed in your system. This link
also lists Windows Subsystem for Linux (WSL) for window 10(Windows 10 Creators or Anniversary Update).
I am using python 3.5.2. Hopefully any version of python 3 should work.

*** creating virtual environment and installing packages for data science tools 1
*Run following commands from  command prompt.*

- *apt-get install python3-venv*
- Using command line(*cd command*), go to the folder where you want to keep python file, notebooks related to this course.
- run *python3 -m venv /path/to/new/virtual/environment*
  + e.g. I ran *python3 -m venv data_mining_env*
- To activate your environment run *source /path/to/new/virtual/environment/bin/activate*
  + e.g From this course directory I run, *source data_mining_env/bin/activate*

- run *python3 -m pip install \-\-upgrade pip*. Note that there are 2 dashes in upgrade option.
- run *wget https://raw.githubusercontent.com/psnegi/data_science_tools1/master/requirements.txt*
- run *pip install -r requirements.txt*
- run *jupyter notebook* or *jupyter lab*. 
- In the browser you should see your current files.
- Click on the notebook you want to run.

- click on *RISE* slideshow extension in notebook, if you want to see notebook as slideshow.

To deactivate  python virtual environment, run *deactivate*

*** Python learning resources
You can also go to my  [[https://github.com/psnegi/PythonForReproducibleResearch][python for reproducible research]]  github repository and start by running pythonBasic.ipynb notebook.
I will go over basic of python and jupyter notebook.

   - [[https://try.jupyter.org/][try python notebook online without installing anything]]
   - [[http://pythontutor.com/live.html#mode%3Dedit][Runs and visualizes your python code]]
   - [[https://docs.python.org/3/tutorial/index.html][The Python Tutorial]]  

* Homeworks
We'll allow 2 late homework with cutoff of 36 hours. We'll give *ceil(total_marks_obtained*exp(-(minutes late)/(24*60)))* marks  for  late submitted assignments via email.

|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|                                   HW no | description and links                                               |                                                             | solution |
|                                         |                                                                     | Due date                                                    |          |
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|                                         | *Written part(please write solution clearly)::*                     | *Written Part:* In class 11 th April                        |          |
|                                         |                                                                     |                                                             |          |
|                                         | (1 point each)                                                      |                                                             |          |
|                                       1 | Chapter1: Q 2(*only if enrolled in COMP 4431*)                      |                                                             |          |
|                                         | Chapter2: Q 1, 4(*only if enrolled in COMP 3431*), 5, 6, 7          | *Jupyter notebook:* Online via canvas 13 th April 11.59 p.m |          |
|                                         | *Note: Only  chapter 1 Q2 and chapter 2 Q4 has a choice depending*  |                                                             |          |
|                                         | *on your enrollment otherwise solve all the assigned the problems.* |                                                             |          |
|                                         |                                                                     |                                                             |          |
|                                         | *Online submission jupyter notebook(use numpy and matplotlib)::*    |                                                             |          |
|                                         | Chapter2: Q 2(.2 point each part), 8(1 point), 10(a)(.2 points)     |                                                             |          |
|                                         |                                                                     |                                                             |          |
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|                                       2 | Solve the problems in the [[./hws/HW_2_coding_PCA.ipynb][pca notebook]]                              | 21 st April 11.59 p.m                                       |          |
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|                                       3 | hw3a Solve the problems in this [[./hws/HW3_LDA.ipynb][FLDA]] notebook                       | hw3a 28 th April 11.59 p.m                                  |          |
|                                         |                                                                     | hw3b 30 th April in the class                               |          |
|                                         | solve problem in [[./hws/hw3b.pdf][hw3b]]                                               |                                                             |          |
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|                                       4 | solve problems in this [[./hws/hw4a_kernel_linear_reg.pdf][hw4a]]                                         | 7 th May 11 a.m. (I'll post solution after 11)              |          |
|                                         | Solve the problem in this [[https://colab.research.google.com/github/psnegi/data_mining/blob/master/hws/Kernel_pca_question.ipynb][kernel PCA hw4b]]                           | 17 May 11.59 p.m (for kernel pca)                           |          |
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
| 5   solve problem in this notebook [[https://colab.research.google.com/github/psnegi/data_mining/blob/master/hws/HW5a_implementing_naive_bayes.ipynb][hw5a]] |                                                                     | Sunday 26 th May 11.59 p.m                                  |          |
|-----------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+----------|
|                                         |                                                                     |                                                             |          |
* Midterm
  - [[./hws/sample_midterm.pdf][practice midterm]]    [[./hws/sample_midterm_sol_key.pdf][solution key]]
* Notebook
**  2 April
  - [[https://mybinder.org/v2/gh/psnegi/data_mining/master?filepath=notebooks/data_mining_introduction.ipynb][data mining intro]]
** 4 April
  -   - [[https://mybinder.org/v2/gh/psnegi/data_mining/master?filepath=notebooks/MVN_demo.ipynb][multi variate gaussian]]
** 9 th May
   [[https://colab.research.google.com/github/psnegi/data_mining/blob/master/notebooks/ridge_regression_code.ipynb][ridge regression]]

** 28 th May
   [[https://colab.research.google.com/github/psnegi/data_mining/blob/master/notebooks/matrix_factroization.ipynb][matrix factoization]]

** 6 th june
 - [[https://colab.research.google.com/github/psnegi/data_mining/blob/master/notebooks/k_mean_agglometrative_clustering.ipynb][sklearn k mean and agglomerative clustering]]
* Course Activity

| Date        | Reading/Coding Assignments                                               | class activity                                                                                                                                  |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 2 - April   | read chapter 1                                                           | Went over Data Matrix view, properties of vector, projections                                                                                   |
|             |                                                                          | *Please review, linear independence, column space, row space, rank of matrix*                                                                   |
|             |                                                                          | *From probability part go over random variables, probability mass function and density function*                                                |
|             |                                                                          | *Go over Bernoulli, Binomial and normal random variable*                                                                                        |
|             | [[https://1drv.ms/o/s!AuJzJXvAm2RThmzwwfh46bqSdtLr][In class scribed notes]]                                                   | If you have trouble creating virtual environment please install anaconda python 3.7 from                                                        |
|             |                                                                          | https://www.anaconda.com/distribution/#download-section                                                                                         |
|             |                                                                          |                                                                                                                                                 |
|             |                                                                          | See video for installation https://www.youtube.com/watch?v=OOFONKvaz0A                                                                          |
|             |                                                                          |                                                                                                                                                 |
|             |                                                                          | To download the notebooks from course website, go inside notebook folder, click on notebook.                                                    |
|             |                                                                          | This  should render static notebook(You can't run it). Click on raw option, then save resulting                                                 |
|             |                                                                          | file in folder. You may want to put "" around file name before saving it, otherwise the extension will be .txt                                  |
|             |                                                                          | Fire up anaconda and use jupyter notebook. You need to load downloaded notebook.                                                                |
|             |                                                                          | *To run the cell in notebook press ctr+alt(it will run the cell and create a new empty cell) or ctr+enter(it will run the cell)*                |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 4 April     | read chapter 2                                                           | Started with the probabilistic view of the attributes and connection with linear algebra  is highlighted.                                       |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 9 th April  | read chapter 7 (ignore kernel PCA)                                       | multi variate random variable and co variance matrix, its property. change of basis formula. Started with PCA motivation.                       |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 11 th April | chapter 7                                                                | PCA derivation and application of re construction for image denoising.                                                                          |
|             |                                                                          | Convex optimization is not required but if you are interested here is the link to [[http://web.stanford.edu/class/ee364a/][Convex Optimization I]]                                         |
|             |                                                                          | by Professor  Stephen Boyd, Stanford University.                                                                                                |
|             |                                                                          |                                                                                                                                                 |
|             |                                                                          | For quick overview see [[https://metacademy.org/graphs/concepts/lagrange_duality#focus%253Dlagrange_multipliers&mode%253Dlearn][metacademy]]                                                                                                               |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 16 th April | Chapter 20  (ignore kernel part)                                         | PCA via SVD, minimizing error in PCA is same as maximizing variance, Derived LDA for two classes,                                               |
|             |                                                                          | Talked about dimensionality of data in the LDA(Fisher Discriminant analysis) space.                                                             |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 18 th April | Chapter 5 (Kernel method)                                                | Finished multi-class LDA. Started with motivation for modifying data to a new features space, kernel functions.                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 23 rd April |                                                                          | Finished kernel, mercer theorem and idea of kernelization. Started with kernel PCA.                                                             |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 25 th April |                                                                          | Finished kernel PCA, idea of centralization matrix $H= I- 1^T 1$ from left and right. Started with linear regression.                           |
|             |                                                                          |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 30 th April |                                                                          | Finished linear regression, started with model regularization(ridge, lasso and elastic net), Will finish kernelization of ridge regression      |
|             |                                                                          |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 2nd May     |                                                                          | Finished model complexity and regularization and kernelizing ridge regression                                                                   |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 7 May       | midterm                                                                  |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 9 th May    |                                                                          | ridge , lasso demo, started logistic regression                                                                                                 |
|             |                                                                          |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 14 th May   | chapter 18 probabilistic classification                                  |                                                                                                                                                 |
|             |                                                                          |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 21 May      | chapter 21                                                               |                                                                                                                                                 |
|             | [[http://2.bp.blogspot.com/_kje1Q4JpgZc/SciiUyj8bdI/AAAAAAAABxM/cllUoYUMkB4/s400/svm.JPG][SVM cartoon]]                                                              |                                                                                                                                                 |
|             | credit: http://maktoons.blogspot.com/2009/03/support-vector-machine.html |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 23 May      |                                                                          | Finished  svm separable and non separable scenario, kernel  svm                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 28 th May   | KNN                                                                      | Finished KNN and its variation. Started Matrix factorization for data mining(topic modelling using SVD)                                         |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 30 th May   | [[https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf][Optional: MATRIX FACTORIZATION TECHNIQUES FOR RECOMMENDER SYSTEMS]]        |                                                                                                                                                 |
|             | Chapter 13 th clustering                                                 |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 4 th May    |                                                                          | Representative(mean etc.) clustering, k mean(better initialization *k mean++*, number of cluster *elbow method*), kernel k mean, E-M clustering |
|             |                                                                          |                                                                                                                                                 |
|-------------+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| 6 th May    | Chapter 22 (hierarchical clustering)                                     | Some open source software: [[https://www.cs.waikato.ac.nz/ml/weka/][weka3, collection of machine learning algorithms for data mining tasks]]                                               |
|             |                                                                          | [[https://www.knime.com/][KNIME: solution for data-driven innovation]]                                                                                                             |
|             |                                                                          | Covered Agglomerative clustering, For  density based clustering read DBSCAN from book                                                           |
|             |                                                                          |                                                                                                                                                 |
