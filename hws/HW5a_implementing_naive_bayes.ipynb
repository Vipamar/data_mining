{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1466096115517-bceecbfb6fde?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=427bcc1d8e2505d31a239d0de6b13f75&auto=format&fit=crop&w=1950&q=80\"  width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This excercise will help us to get started with **navie bayes  classifier** and using tools(pandas, sklearn etc) from python eco system.\n",
    "\n",
    "**Problem statement:** classify SMS messages as *HAM* or *SPAM* using **naive bayes** in supervised machine setting.\n",
    "See this link to get an idea supervised learning workflow [supervsed learning workflow](http://www.allprogrammingtutorials.com/tutorials/introduction-to-machine-learning.php)\n",
    "\n",
    "**Dataset:** We will use [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from UCI machine learning repository.\n",
    "\n",
    "credit:\n",
    "\n",
    "\n",
    "- some of the images are from https://cdn.pixabay.com\n",
    "- https://unsplash.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook may require installation of gensim  NLP(text processing) library\n",
    "\n",
    "<font color=\"red\"> To anwser questions, look for write your code in the notebook </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output should be 0 after successful install\n",
    "# run this only once. Comment later\n",
    "os.system('pip install gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must for inline plot\n",
    "%matplotlib inline \n",
    "import requests\n",
    "import numpy as np\n",
    "import pprint # for pretty printing\n",
    "import os # listing and managing file patho\n",
    "import zipfile # for zip and unzip utilities\n",
    "import pandas # for data analysis\n",
    "import csv\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(data_url)\n",
    "#r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and save the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_zip_file = 'smsspamcollection.zip'\n",
    "with open(sms_zip_file, 'wb') as out_file:\n",
    "    out_file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's verify it. \n",
    "**make sure output of following command contains smsspamcollection.zip file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quiz1_18april_2019.tex', 'quiz1_18april_2019.tex~', 'HW_2_coding_PCA.ipynb', 'Kernel_pca.ipynb', 'implementing_naive_bayes.ipynb', 'PCA_demo_sol.ipynb', 'quiz1_18april_2019.log', 'Kernel_pca_question.ipynb', '.ipynb_checkpoints', 'quiz1_18april_2019.pdf', 'HW3_LDA.ipynb', 'smsspamcollection.zip']\n"
     ]
    }
   ],
   "source": [
    "#Let verify it. \n",
    "dir_listing = os.listdir('.') # list content of current directory\n",
    "print(dir_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(sms_zip_file,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's list the content of the new data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read  dataset readme file for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMS Spam Collection v.1\r",
      "\r\n",
      "-------------------------\r",
      "\r\n",
      "\r",
      "\r\n",
      "1. DESCRIPTION\r",
      "\r\n",
      "--------------\r",
      "\r\n",
      "\r",
      "\r\n",
      "The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. \r",
      "\r\n",
      "\r",
      "\r\n",
      "1.1. Compilation\r",
      "\r\n",
      "----------------\r",
      "\r\n",
      "\r",
      "\r\n",
      "This corpus has been collected from free or free for research sources at the Web:\r",
      "\r\n",
      "\r",
      "\r\n",
      "- A collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/\r",
      "\r\n",
      "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf\r",
      "\r\n",
      "- A subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/\r",
      "\r\n",
      "- The amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by Jos� Mar�a G�mez Hidalgo and public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "1.2. Statistics\r",
      "\r\n",
      "---------------\r",
      "\r\n",
      "\r",
      "\r\n",
      "There is one collection:\r",
      "\r\n",
      "\r",
      "\r\n",
      "- The SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "1.3. Format\r",
      "\r\n",
      "-----------\r",
      "\r\n",
      "\r",
      "\r\n",
      "The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\r",
      "\r\n",
      "\r",
      "\r\n",
      "ham   What you doing?how are you?\r",
      "\r\n",
      "ham   Ok lar... Joking wif u oni...\r",
      "\r\n",
      "ham   dun say so early hor... U c already then say...\r",
      "\r\n",
      "ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\r",
      "\r\n",
      "ham   Siva is in hostel aha:-.\r",
      "\r\n",
      "ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\r",
      "\r\n",
      "spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\r",
      "\r\n",
      "spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\r",
      "\r\n",
      "spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\r",
      "\r\n",
      "\r",
      "\r\n",
      "Note: messages are not chronologically sorted.\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "2. USAGE\r",
      "\r\n",
      "--------\r",
      "\r\n",
      "\r",
      "\r\n",
      "We offer a comprehensive study of this corpus in the following paper that is under review. This work presents a number of statistics, studies and baseline results for several machine learning methods.\r",
      "\r\n",
      "\r",
      "\r\n",
      "[1] Almeida, T.A., G�mez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "3. ABOUT\r",
      "\r\n",
      "--------\r",
      "\r\n",
      "\r",
      "\r\n",
      "The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) and Jos� Mar�a G�mez Hidalgo (http://www.esp.uem.es/jmgomez).\r",
      "\r\n",
      "\r",
      "\r\n",
      "We would like to thank Dr. Min-Yen Kan (http://www.comp.nus.edu.sg/~kanmy/) and his team for making the NUS SMS Corpus available. See: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. He is currently collecting a bigger SMS corpus at: http://wing.comp.nus.edu.sg:8080/SMSCorpus/\r",
      "\r\n",
      "\r",
      "\r\n",
      "4. LICENSE/DISCLAIMER\r",
      "\r\n",
      "---------------------\r",
      "\r\n",
      "\r",
      "\r\n",
      "We would appreciate if:\r",
      "\r\n",
      "\r",
      "\r\n",
      "- In case you find this corpus useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\r",
      "\r\n",
      "- Send us a message to tiago@dt.fee.unicamp.br in case you make use of the corpus.\r",
      "\r\n",
      "\r",
      "\r\n",
      "The SMS Spam Collection v.1 is provided for free and with no limitations excepting:\r",
      "\r\n",
      "\r",
      "\r\n",
      "1. Tiago Agostinho de Almeida and Jos� Mar�a G�mez Hidalgo hold the copyrigth (c) for the SMS Spam Collection v.1.\r",
      "\r\n",
      "\r",
      "\r\n",
      "2. No Warranty/Use At Your Risk. THE CORPUS IS MADE AT NO CHARGE. ACCORDINGLY, THE CORPUS IS PROVIDED `AS IS,' WITHOUT WARRANTY OF ANY KIND, INCLUDING WITHOUT LIMITATION THE WARRANTIES THAT THEY ARE MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. YOU ARE SOLELY RESPONSIBLE FOR YOUR USE, DISTRIBUTION, MODIFICATION, REPRODUCTION AND PUBLICATION OF THE CORPUS AND ANY DERIVATIVE WORKS THEREOF BY YOU AND ANY OF YOUR SUBLICENSEES (COLLECTIVELY, `YOUR CORPUS USE'). THE ENTIRE RISK AS TO YOUR CORPUS USE IS BORNE BY YOU. YOU AGREE TO INDEMNIFY AND HOLD THE COPYRIGHT HOLDERS, AND THEIR AFFILIATES HARMLESS FROM ANY CLAIMS ARISING FROM OR RELATING TO YOUR CORPUS USE.\r",
      "\r\n",
      "\r",
      "\r\n",
      "3. Limitation of Liability. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR THEIR AFFILIATES, OR THE CORPUS CONTRIBUTING EDITORS, BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES, INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF ADVISED OF THE POSSIBILITY THEREOF, AND REGARDLESS OF WHETHER ANY CLAIM IS BASED UPON ANY CONTRACT, TORT OR OTHER LEGAL OR EQUITABLE THEORY, RELATING OR ARISING FROM THE CORPUS, YOUR CORPUS USE OR THIS LICENSE AGREEMENT.\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMSSpamCollection file contains around 5k SMS messages. Checkout readme file for details.\n",
    "\n",
    "**Let's open this file and store line in python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('./data/SMSSpamCollection', 'r') as f:\n",
    "    sms_messages = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sms messages is 5574\n"
     ]
    }
   ],
   "source": [
    "# Following code show how to write list comprehension. We could have done this using for loop too.\n",
    "# [<some_func>(x) for x in <something> if  <some_condition_is_true>]\n",
    "sms_messages = [m.rstrip() for m in sms_messages] # we are not using if condition part\n",
    "print('Number of sms messages is {}'.format(len(sms_messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check couple of messages again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message id 0  ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "message id 1  ham\tOk lar... Joking wif u oni...\n",
      "message id 2  spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "message id 3  ham\tU dun say so early hor... U c already then say...\n",
      "message id 4  ham\tNah I don't think he goes to usf, he lives around here though\n",
      "message id 5  spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "message id 6  ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "message id 7  ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "message id 8  spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "message id 9  spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "message id 10  ham\tI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "message id 11  spam\tSIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "message id 12  spam\tURGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "message id 13  ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n",
      "message id 14  ham\tI HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "message id 15  spam\tXXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
      "message id 16  ham\tOh k...i'm watching here:)\n",
      "message id 17  ham\tEh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
      "message id 18  ham\tFine if thats the way u feel. Thats the way its gota b\n",
      "message id 19  spam\tEngland v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+\n"
     ]
    }
   ],
   "source": [
    "for idx, msg in enumerate(sms_messages[0:20]): # see how we can slice list using : operator\n",
    "    print('message id {}  {}'.format(idx, msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is our  data set $\\mathcal{D} = \\{({x_i}, y_i)\\}_{i=1}^{N=5574}$ $x_i$ is sms message and $y_i$ is label(ham or spam)**. Using  this we will train(learn parameters $\\theta$ of a models(Naive bayes etc.)) and use trained model to classify new messages as ham or spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step before jumping into using any machine learning model is understanding the data by **describing it's statistical attribute and visualizating samples or sample property**.\n",
    "We can use CSV file reader and try to accomplish above task. But as they say python is a language with **battery(libraries) included**. Let's use **pandas and matplotlib** libraries to do this task as cleanly as possible. What to describe and what to plot will be an essential skill we build as we do various data science or machine learning tasks. Also with time you will also built a knowledge of various packages available for different domain in python eco system. Most of the time reading blog and google search does the job of finding right libraries. Various packages for download and installation are avaiable at [PyPI - the Python Package Index](https://pypi.python.org/pypi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**\n",
    "This is [10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "\n",
    "If you have more time look into this link [Pandas Tutorial: DataFrames in Python](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python#gs.dEdNuDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will see how wrapping the file in pandas simplify lot of tasks\n",
    "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head(6) # there are other functions like tail and sample to check record in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to understand various attribute of the data\n",
    "\n",
    "*How many messages in each group etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see  spam class has less number of example than ham class. This is called **class imbalance** issue. \n",
    "- We need to be carfeful in machine learning application about class imbalance. There are way to handle it but for current exercise let's ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1(.5 point). Can you suggest some way to handle class imbalance issue. This is an open question. So don't worry about  being right or wrong. I am looking for some fresh thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*write you answer in this cell (doble click and it will be editable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer only understand scalar or vector or matrices. We need to convert text to vectors(features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "(It was a big  sought after skill before deep learning at least in vision applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) approach for creating feature\n",
    "representing our sms messages.\n",
    "\n",
    "### Bag of word model for document:\n",
    "\n",
    "In BOG  we treat document as collection of word without any order, like they are lying in bag. \n",
    "\n",
    "Two model to represent sms/document in a vector form are:\n",
    "\n",
    "- **Bernoulli document model: mes**sage is represented by a binary feature vector of absence or presence of word.\n",
    "- **Multinomial document model**: message is represented by an integer feature vector of word frequency.\n",
    "\n",
    "\n",
    "\n",
    "We will use **Multinomial document model** in this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to vector is bit involved and require a good understanding of NLP(natural language processing).\n",
    "\n",
    "But as we can imagine to convert a message into vector we need to\n",
    "1. convert a sentence into word token\n",
    "2. Normalize the words i.e do we care about Capital form(Cow vs cow), inflected form (\"goes\" vs. \"go\")\n",
    "3. Build a dictionary of words and map the messages into vector using this dictionary\n",
    "4. Finally train a  Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use a python library [Gensim](https://radimrehurek.com/gensim/tutorial.html) to do heavy lifting for us.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_messages = []\n",
    "for c in messages.message:\n",
    "    preprocessed_messages.append(gensim.utils.simple_preprocess(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_messages=  np.array(preprocessed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
     ]
    }
   ],
   "source": [
    "for m in messages.message[0:3]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'great', 'world', 'la', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']),\n",
       "       list(['ok', 'lar', 'joking', 'wif', 'oni']),\n",
       "       list(['free', 'entry', 'in', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', 'over'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how basic clean up of messages is done using gensim **simple_preprocess**. We can do some more pre processing  with some justification but let's go\n",
    "ahead with current manipulation.\n",
    "\n",
    "There are lots of other natural language processing libraries in python for performing above activities like\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [nltk](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the fundamental concern in machine learning is *Generalization*, **how well our machine is going to work well on unseen/future data. Does it generalize well on future data**?\n",
    "- If we wanted to do well on given data, why  would we even bother to build an algorithm. We can just store the data and do a lookup for any sms message.\n",
    "\n",
    "We will try to come back to this question later in the course when we talk about **algorithm/model selection and evaluation**.\n",
    "\n",
    "One simple way to answer above question is to hide some portion of dataset and use remaining dataset for building the model i.e. learning the parameters. Once we have build the model, we can report some number/measure on hidden dataset to tell how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's partition our data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_labels = messages.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples are 5574\n",
      "train set size is 5016 test set size is 558\n"
     ]
    }
   ],
   "source": [
    "training_set_portion =.9 # keep 90 % data for training\n",
    "#LEt's create some random integer index and partition the data\n",
    "number_of_examples = len(preprocessed_messages)\n",
    "print('Total examples are {}'.format(number_of_examples))\n",
    "np.random.seed(0) # to make sure multiple run give same result\n",
    "random_index = np.random.permutation(range(number_of_examples))\n",
    "training_set_size = int(number_of_examples*training_set_portion)\n",
    "print('train set size is {} test set size is {}'.format(training_set_size,number_of_examples - training_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['ok', 'lar', 'joking', 'wif', 'oni']),\n",
       "       list(['free', 'entry', 'in', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', 'over']),\n",
       "       list(['dun', 'say', 'so', 'early', 'hor', 'already', 'then', 'say'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X (5016,) and train Y (5016,)\n",
      "Shape of test X (558,) and test Y (558,)\n"
     ]
    }
   ],
   "source": [
    "training_messages = preprocessed_messages[random_index[:training_set_size]]\n",
    "training_labels = messages_labels[random_index[:training_set_size]]\n",
    "test_messages = preprocessed_messages[random_index[training_set_size:]]\n",
    "test_labels = messages_labels[random_index[training_set_size:]]\n",
    "\n",
    "print('Shape of training X {} and train Y {}'.format(training_messages.shape, training_labels.shape))\n",
    "print('Shape of test X {} and test Y {}'.format(test_messages.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use training messages only for building the model**\n",
    "\n",
    "- We need to convert each message into count vector. Where a ham/spam message is mapped to vector representing each word frequency in the message\n",
    "    + To do this we need to build a dictionary of words first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 2(.5 point) Write code to update the python set. We are using set as it takes care of duplicate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words are in our dictionary\n",
    "unique_word = set()\n",
    "for message in training_messages:\n",
    "    # write your code here to update unique_word set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7353"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in vocabulary(V)\n",
    "len(unique_word)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode each message into len(unique_word) dimensional vector. In meachine learning we call acitivity like this feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let' use default dictionary to assign each word a unique location in feature vector\n",
    "from collections import defaultdict, Counter\n",
    "word_to_index_dict = defaultdict(int)\n",
    "for index , word in enumerate(unique_word):\n",
    "    word_to_index_dict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a reverse dictionary  for mapping index to word. It will help in debugging etc.\n",
    "# See how we used dictionary comprehension\n",
    "index_to_word_dict = { value:key  for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016,)\n"
     ]
    }
   ],
   "source": [
    "print(training_messages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will convert each  messages into |V| dimensional vector, where |V| is size of our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a numpy integer matrix of right size, initialized with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016, 7353)\n"
     ]
    }
   ],
   "source": [
    "# each row in training_X is our x_i\n",
    "training_X = np.zeros((len(training_messages), len(unique_word)), dtype=int)\n",
    "print(training_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 (.5 point) use [Counter](https://docs.python.org/3.5/library/collections.html#collections.Counter) from collections to count frequency of each word in a message. I have written rest of the code for updating frequency into right index of the feature vector.\n",
    "\n",
    "<font color=\"red\"> Look for write your code here </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go over each training message, count the words using Counter and set count in feature vector for sms \n",
    "\n",
    "for sms_no, sms in enumerate(training_messages):\n",
    "    word_freq =  # write your code here\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        index_of_word = word_to_index_dict[word]\n",
    "        training_X[sms_no][index_of_word] = freq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing code is easy :)\n",
    "## But how we check if it is correct\n",
    "## Let's do some primitive checking on a sms message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_no =3\n",
    "message_word_count = Counter(training_messages[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(training_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red' > Make sure Counter and encoding gives same results in above cell </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have successfully converted sms message into feature vector and\n",
    "# collected them in numpy matrix\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1522098543979-ffc7f79a56c4?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3deb7fa95bb0a7343a38b724cbee4b5a&auto=format&fit=crop&w=1868&q=80\" alt=\"Well done\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's convert ham and spam label to 1 and 0  respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels.tail(7)# can check from head too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our training_y value\n",
    "training_y = (training_labels.values == 'ham').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check some lable value\n",
    "training_y[-7:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model or estimating parameters $\\theta$ of the model\n",
    "Now we have vector feature representation $x_i$ of our sms samples. \n",
    "\n",
    "Let review some theory and see what parameters we need to estimate for Naive bayes model.\n",
    "\n",
    "We know that we classify a sms $x_i$  to a class c= HAM or c= SPAM which has maximum value of $P(c|x_i).$ Using bayes rule we have $P(c|x_i) = \\frac{P(x_i|c) P(c)}{P(x_i)} \\propto P(x_i|c) P(c)$ as normalization doesn't depend on class label. \n",
    "\n",
    "In naive bayes assumption for modelling class conditional densities we have $P(x_i|c) = \\prod_j^D P(x_{ij}|c)$ assuming  $x_i \\in \\mathbb{R}^D$ i.e each example has $D$ dimentional features.\n",
    "\n",
    "**Note:$D$ is size of our vacabulary ($|V|$) build from sms document corpus i.e D = |V|**\n",
    "\n",
    "**what probability distribution we should choose for $P(x_{ij}|c)?$ **\n",
    "\n",
    "Each value $x_{ij}$ is an integer values(count of words) and there are total $D$ different unique values(word). This definetly suits a **$D$ side die** situation. In our case die has $D = |V|$ sides= size of feature vector.\n",
    "\n",
    "**Infact once we have learned $P(x_{ij}|c)?$ i.e probabilites of different sides for ham and spam die,**\n",
    "** ham or spam sms generation in bag of word model is nothing but rolling ham or spam die. Pick the word dictated by the side of die throw.**\n",
    "\n",
    "Now we  know that we can put multinomial distribution for such situation. Hence\n",
    "<font size = 6> \n",
    "$P(x_i|c) = \\frac{n !}{\\prod_j^D x_{ij !}} P(c) \\prod^{D} P(w_j|c)^{x_{ij}} \\propto P(c) \\prod^{D} P(w_j|c)^{x_{ij}}$ \n",
    "</font>\n",
    "as normalization doesn't depend on class label\n",
    "\n",
    "We know that using MLE estimate we have\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c)}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c)}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function.\n",
    "\n",
    "\n",
    "- Hence the parameters are nothing  nothing but relative frequency of $w_j$ in documents of class c=SPAM or c= HAM\n",
    "with respect to the total number of words in documents of that class.\n",
    "\n",
    "- We can sum our numpy sms_feature matrix along row or dim 0 to get total frequency of each feature for ham and spam class\n",
    "- normalize total frequency of each feature with total frequency of all the features for each class.\n",
    "- prior class  densites are estimated as $P(c) = \\frac{N_c}{N}.$ Where $N_c$ are numer of document in class k.\n",
    "\n",
    "\n",
    "\n",
    "# Let's learn the parameters for c= ham(1) and c= spam(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ham class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First estimate for ham\n",
    "\n",
    "# summing up per feature count\n",
    "training_X_ham = training_X[training_y ==1]\n",
    "print(training_X_ham.shape)\n",
    "per_feature_count =np.sum(training_X_ham, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_ham = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's estimate parameters for spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 4(1 Point): Write code similary to ham class to estimate following parameter parameters_w_spam for spam class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "\n",
    "print(parameters_w_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero probability issue\n",
    "As we can see some of the probablity can be zero. It will create problem when we estimate probability of a new document in test set if that word was not in training set. \n",
    "\n",
    "If any of the term in product is zero it will result in zero product. If any of the class don't have this term then probability of this document for any class will be zero. It is an ambiguous situation. If we play log trick for comparing product of probability, we will be in troble as log of zero is not defined too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to handle this situtation to add a fake 1 count of the word in each class. This is called Laplace law of sccession or add one smoothing.\n",
    "\n",
    "We estimate\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c) + 1}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c) + |V|}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function and $|V|$ is size of our dictionary.\n",
    "\n",
    "This can be done by adding a row of ones to training_X_ham and training_X_spam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New parameters Laplace law of sccession or add one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 5 (2 point) Estimate new parameter for ham class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "print(parameters_w_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 6(1 point). Estimate new parameter for spam class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "print(parameters_w_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 7 (1 point) estimate class probabilities P(c=ham) and P(c=spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = # Write your code here for estimating ham class probability\n",
    "spam = 1- ham\n",
    "ham,spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have learned the model(i.e its parameters, probabilities of different words occuring in ham dice and spam dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is our model ?\n",
    "- Let take out our test data and convert to count feature vector using same dictionary\n",
    "- Calulate the probability if test data belonging to Ham or spam. i.e if probability if >=.5 Ham otherwise spam\n",
    " or we can calulate the ratio\n",
    " <font size = 5>\n",
    " $\\frac{P(x_{test}|c=ham)}{P(x_{test}|c=spam)} = \\frac{ P(c=ham)  \\prod^{D}_{j =1} P(w_j|c=ham)^{x_{test,j}}} { P(c= spam)\\prod^{D}_{j=1} P(w_j|c=spam)^{x_{test, j}}}$ \n",
    " </font>\n",
    " \n",
    " **Note:Generally such large product of probabilties, turns out to be zero because of computer representation limits of real numbers.**\n",
    " \n",
    " Another option is let take log on right hand side and after some manipulation one can show that if\n",
    " <font size = 5>\n",
    "  $\\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=ham)) +log(P(c= ham)) \\ge log(P(c=spam))+ \\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=spam))$\n",
    "  \n",
    "  </font>\n",
    "  \n",
    " then it is ham otherwise spam\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "- Compare our prediction of label with test data label and let's report accuracy\n",
    "\n",
    "Let's do these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.zeros((len(test_messages), len(unique_word)), dtype=int)\n",
    "print(test_X.shape)\n",
    "len(word_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(sms, word_to_index_dict):\n",
    "    feature = np.zeros((len(word_to_index_dict),), dtype=int)\n",
    "    word_freq =  Counter(sms)\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in word_to_index_dict:\n",
    "            index_of_word = word_to_index_dict[word]\n",
    "            feature[index_of_word] = freq\n",
    "    return feature        \n",
    "    \n",
    "for sms_no, sms in enumerate(test_messages):\n",
    "    test_X[sms_no] = build_feature(sms, word_to_index_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again checking feature creation/encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_no =2\n",
    "message_word_count = Counter(test_messages.iloc[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(test_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need test_y  value to compare predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 8 (.5 point) convert ham and spam to 1 and 0 integer as done in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our test_y value\n",
    "test_y = # Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally let's calculate ham/spam probability for test messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_score = np.zeros_like(test_y,dtype=float)\n",
    "spam_score = np.zeros_like(test_y,dtype=float)\n",
    "ham_score.shape, spam_score.shape# just printing to make sure shape is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(parameters,test_sms, class_prior):\n",
    "    return np.sum(np.log(np.power(parameters,test_sms))) + class_prior\n",
    "\n",
    "for idx, test_sms in enumerate(test_X):# this will fetch row by row, encoded test messages\n",
    "    ham_score[idx] = calculate_score(parameters_w_ham,test_sms, np.log(ham))\n",
    "    spam_score[idx] = calculate_score(parameters_w_spam, test_sms, np.log(spam))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let print some values for visual comparision/verification\n",
    "ham_score[0:2], spam_score[0:2], test_y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the label ham(1) or spam(0)\n",
    "ham_or_spam = (ham_score >= spam_score).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_or_spam[0:5], test_y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 9(.5 point). Write code to calculate accuracy. Accuracy is defined as average number of correct predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = # Write your code here\n",
    "\n",
    "print('accuracy on test set is {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ham_or_spam(message):\n",
    "    feature = build_feature(message, word_to_index_dict)\n",
    "    ham_score = calculate_score(parameters_w_ham,feature, np.log(ham))\n",
    "    spam_score = calculate_score(parameters_w_spam, feature, np.log(spam))\n",
    "    \n",
    "    return 'ham' if ham_score > spam_score else 'spam'\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how it works on new spam message which is a modified  training message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ham_or_spam(' your mailbox messaging sm alert call back 09056242159 to retrieve your message'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Try some messages with same distribution as training messages to see how well it does </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Following code will show why python eco system shines.\n",
    "We will use python library sklearn to build  multinomial Naive Bayes classifier </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 10 (1 = .5 +.5 point) Write code here to train MultinomialNB classifer  from sklearn and report accuracy in test set\n",
    "Your code should not be more than 3 lines. Check if you get same accuracy number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 11(.5) Train LogisticRegression classifier from sklearn and report accuracy in test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
